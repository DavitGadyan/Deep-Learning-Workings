{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'E:\\PYTHON\\Deep_Learning_Frameworks\\PyTorch\\saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4665, 0.7896, 0.7711],\n",
      "        [0.8781, 0.2633, 0.3492],\n",
      "        [0.6115, 0.0886, 0.5632]])\n",
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Create random tensor of size 3 by 3\n",
    "your_first_tensor = torch.rand(3, 3)\n",
    "\n",
    "# Calculate the shape of the tensor\n",
    "tensor_size = your_first_tensor.shape\n",
    "\n",
    "# Print the values of the tensor and its shape\n",
    "print(your_first_tensor)\n",
    "print(tensor_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix of ones with shape 3 by 3\n",
    "tensor_of_ones = torch.ones(3, 3)\n",
    "\n",
    "# Create an identity matrix with shape 3 by 3\n",
    "identity_tensor = torch.eye(3)\n",
    "\n",
    "# Do a matrix mulitplication of tensor_of_ones with identity_tensor\n",
    "matrices_multiplied = torch.matmul(tensor_of_ones, identity_tensor)\n",
    "print(matrices_multiplied)\n",
    "\n",
    "# Do an element-wise multiplication of tensor_of_ones with identity_tensor\n",
    "element_multiplication = tensor_of_ones * identity_tensor\n",
    "print(element_multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_of_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identity_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_of_ones * identity_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(tensor_of_ones,identity_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1252)\n"
     ]
    }
   ],
   "source": [
    "# Initialize tensors x, y and z\n",
    "x = torch.rand(1000, 1000)\n",
    "y = torch.rand(1000, 1000)\n",
    "z = torch.rand(1000, 1000)\n",
    "\n",
    "# Multiply x with y\n",
    "q = x * y\n",
    "\n",
    "# Multiply elementwise z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "print(mean_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = z * q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f'(q) = z * f' (derivate of f with respect to q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df/dq = f'(qz)*(qz)'\n",
    "\n",
    "####(qz)' = q'*z+ q*z' (total derivative of f)\n",
    "####(qz)' = z (with respect to q) (because q is variable >> q'=1, z is constant)\n",
    "\n",
    "f'(qz) = 1 (with respect to f)\n",
    "\n",
    "f'(z) = q * f' (derivate of f with respect to z)\n",
    "####(qz)' = z (with respect to z) (because z is variable >> z'=1, q is constant)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "q = x + y "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q'(x) = q'(x+y) * (x+y)' (with respect to x)\n",
    "\n",
    "q'(x+y) (derivate of f with respect to q) (-2)\n",
    "(x+y)' -derivate of q with repect to x\n",
    "\n",
    "(x+y)' = x'+y' (with respect to x)\n",
    "x' (with respect to x)=1\n",
    "y' (with respect to x) = 0 (cause y constant in tis case)\n",
    "\n",
    "\n",
    "q'(y) = q'(x+y) * (x+y)' (with respect to y)\n",
    "\n",
    "(x+y)' -derivate of q with repect to y\n",
    "\n",
    "(x+y)' = x'+y' (with respect to y)\n",
    "x' (with respect to y)=0 (cause x constant in tis case)\n",
    "y' (with respect to y) = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x is: tensor(5.)\n",
      "Gradient of y is: tensor(5.)\n",
      "Gradient of z is: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Initialize x, y and z to values 4, -3 and 5\n",
    "x = torch.tensor(4., requires_grad=True)\n",
    "y = torch.tensor(-3., requires_grad=True)\n",
    "z = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "# Set q to sum of x and y, set f to product of q with z\n",
    "q = x+ y\n",
    "f = q * z\n",
    "\n",
    "# Compute the derivatives\n",
    "f.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradient of x is: \" + str(x.grad))\n",
    "print(\"Gradient of y is: \" + str(y.grad))\n",
    "print(\"Gradient of z is: \" + str(z.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient of x is: tensor([[0.0003, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0003,  ..., 0.0003, 0.0002, 0.0003],\n",
      "        [0.0003, 0.0002, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n",
      "        ...,\n",
      "        [0.0003, 0.0002, 0.0003,  ..., 0.0003, 0.0002, 0.0003],\n",
      "        [0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n",
      "        [0.0003, 0.0002, 0.0002,  ..., 0.0003, 0.0002, 0.0003]])\n",
      "Gradient of y is: tensor([[0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0003, 0.0003,  ..., 0.0002, 0.0002, 0.0003],\n",
      "        [0.0003, 0.0003, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0003, 0.0002, 0.0003,  ..., 0.0002, 0.0002, 0.0002]])\n",
      "Gradient of z is: tensor([[0.0003, 0.0003, 0.0003,  ..., 0.0003, 0.0003, 0.0003],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        ...,\n",
      "        [0.0002, 0.0003, 0.0002,  ..., 0.0002, 0.0003, 0.0002],\n",
      "        [0.0002, 0.0002, 0.0002,  ..., 0.0002, 0.0002, 0.0002],\n",
      "        [0.0002, 0.0003, 0.0002,  ..., 0.0002, 0.0002, 0.0002]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tensors x, y and z\n",
    "x = torch.rand(1000, 1000, requires_grad=True)\n",
    "y = torch.rand(1000, 1000, requires_grad=True)\n",
    "z = torch.rand(1000, 1000, requires_grad=True)\n",
    "\n",
    "\n",
    "\n",
    "# Multiply tensors x and y\n",
    "q = torch.matmul(x,y)\n",
    "\n",
    "# Elementwise multiply tensors z with q\n",
    "f = z * q\n",
    "\n",
    "mean_f = torch.mean(f)\n",
    "\n",
    "# Calculate the gradients\n",
    "mean_f.backward()\n",
    "\n",
    "# Print the gradients\n",
    "print(\"Gradient of x is: \" + str(x.grad))\n",
    "print(\"Gradient of y is: \" + str(y.grad))\n",
    "print(\"Gradient of z is: \" + str(z.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([17428.5156, 18996.5625, 19615.3301, 18233.5449, 18747.1621, 17264.9355,\n",
      "        18923.6602, 17123.3770, 18917.9512, 18700.2441])\n"
     ]
    }
   ],
   "source": [
    "input_layer =  torch.rand(28 * 28)\n",
    "# Initialize the weights of the neural network\n",
    "weight_1 = torch.rand(784, 200)\n",
    "weight_2 = torch.rand(200, 10)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Multiply hidden_1 with weight_2\n",
    "output_layer = torch.matmul(hidden_1, weight_2)\n",
    "print(output_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate all 2 linear layers  \n",
    "        self.fc1 = nn.Linear(784, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Use the instantiated layers and return x\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.4664, 4.4610, 7.2896, 7.5203])\n",
      "tensor([5.4664, 4.4610, 7.2896, 7.5203])\n"
     ]
    }
   ],
   "source": [
    "input_layer =  torch.rand(2 * 2)\n",
    "weight_1 = torch.rand(4,4)\n",
    "weight_2 = torch.rand(4,4)\n",
    "weight_3 = torch.rand(4,4)\n",
    "# Calculate the first and second hidden layer\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
    "\n",
    "# Calculate the output\n",
    "print(torch.matmul(hidden_2, weight_3))\n",
    "\n",
    "# Calculate weight_composed_1 and weight\n",
    "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
    "weight = torch.matmul(weight_composed_1, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1672, 1.6497, 1.6682, 2.3201])\n",
      "tensor([1.1827, 1.6645, 1.6802, 2.3494])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "relu = nn.ReLU()\n",
    "input_layer =  torch.rand(2 * 2) - 0.1\n",
    "weight_1 = torch.rand(4,4)\n",
    "weight_2 = torch.rand(4,4) - 0.2\n",
    "weight_3 = torch.rand(4,4)\n",
    "\n",
    "# Apply non-linearity on hidden_1 and hidden_2\n",
    "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
    "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
    "print(torch.matmul(hidden_2_activated, weight_3))\n",
    "\n",
    "# Apply non-linearity in the product of first two weights. \n",
    "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
    "\n",
    "# Multiply `weight_composed_1_activated` with `weight_3\n",
    "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
    "\n",
    "# Multiply input_layer with weight\n",
    "print(torch.matmul(input_layer, weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9705, 1.8499])\n"
     ]
    }
   ],
   "source": [
    "input_layer =  torch.rand(2 * 2) - 0.1\n",
    "# Instantiate ReLU activation function as relu\n",
    "relu = nn.ReLU()\n",
    "\n",
    "# Initialize weight_1 and weight_2 with random numbers\n",
    "weight_1 = torch.rand(4, 6)\n",
    "weight_2 = torch.rand(6, 2)\n",
    "\n",
    "# Multiply input_layer with weight_1\n",
    "hidden_1 = torch.matmul(input_layer, weight_1)\n",
    "\n",
    "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
    "hidden_1_activated = relu(hidden_1)\n",
    "print(torch.matmul(hidden_1_activated, weight_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the scores and ground truth\n",
    "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
    "ground_truth = torch.tensor([2])\n",
    "\n",
    "# Instantiate cross entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute and print the loss\n",
    "loss = criterion(logits,ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.1975)\n"
     ]
    }
   ],
   "source": [
    "# Import torch and torch.nn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize logits and ground truth\n",
    "logits = torch.rand(1,1000)\n",
    "ground_truth = torch.tensor([111])\n",
    "\n",
    "# Instantiate cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Calculate and print the loss\n",
    "loss = criterion(logits,ground_truth)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Transform the data to torch tensors and normalize it \n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "\t\t\t\t\t\t\t\ttransforms.Normalize((0.1307), ((0.3081)))])\n",
    "\n",
    "# Prepare training set and testing set\n",
    "trainset = torchvision.datasets.MNIST('mnist', train=True, \n",
    "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST('mnist', train=False,\n",
    "\t\t\t   download=True, transform=transform)\n",
    "\n",
    "# Prepare training loader and testing loader\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
    "\t\t\t\t\t\t\t\t\t\t shuffle=False, num_workers=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaya\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:53: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n",
      "C:\\Users\\Gaya\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:58: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) torch.Size([10000, 28, 28])\n",
      "32 32\n"
     ]
    }
   ],
   "source": [
    "# Compute the shape of the training set and testing set\n",
    "trainset_shape = trainloader.dataset.train_data.shape\n",
    "testset_shape = testloader.dataset.test_data.shape\n",
    "\n",
    "# Print the computed shapes\n",
    "print(trainset_shape, testset_shape)\n",
    "\n",
    "# Compute the size of the minibatch for training set and testing set\n",
    "trainset_batchsize = trainloader.batch_size\n",
    "testset_batchsize = testloader.batch_size\n",
    "\n",
    "# Print sizes of the minibatch\n",
    "print(trainset_batchsize, testset_batchsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the class Net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):    \n",
    "    \t# Define all the parameters of the net\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):   \n",
    "    \t# Do the forward pass\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
    "optimizer = optim.Adam(model.parameters(),lr=3e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "  \n",
    "for batch_idx, data_target in enumerate(train_loader):\n",
    "    data = data_target[0]\n",
    "    target = data_target[1]\n",
    "    data = data.view(-1, 28 * 28)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Complete a forward pass\n",
    "    output = model(data)\n",
    "\n",
    "    # Compute the loss, gradients and change the weights\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Set the model in eval mode\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(test_loader, 0):\n",
    "    inputs, labels = data\n",
    "    \n",
    "    # Put each image into a vector\n",
    "    inputs = inputs.view(-1, 28 * 28)\n",
    "    \n",
    "    # Do the forward pass and get the predictions\n",
    "    outputs = model(inputs)\n",
    "    _, outputs = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (outputs == labels).sum().item()\n",
    "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /files/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                              | 0/9912422 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                              | 16384/9912422 [00:00<01:37, 101292.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|                               | 24576/9912422 [00:00<02:01, 81177.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "  0%|▏                              | 49152/9912422 [00:00<01:43, 95301.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "  1%|▏                             | 73728/9912422 [00:00<01:29, 109609.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                             | 90112/9912422 [00:01<01:28, 111100.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                            | 114688/9912422 [00:01<01:20, 122033.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "  1%|▍                            | 139264/9912422 [00:01<01:14, 130961.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "  2%|▍                            | 155648/9912422 [00:01<01:16, 128086.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "  2%|▌                            | 180224/9912422 [00:01<01:10, 138373.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "  2%|▌                            | 196608/9912422 [00:01<01:18, 123773.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                            | 221184/9912422 [00:01<01:12, 133767.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "  2%|▋                            | 237568/9912422 [00:02<01:16, 125867.80it/s]\n",
      "\n",
      "\n",
      "\n",
      "  3%|▋                            | 253952/9912422 [00:02<01:19, 121139.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "  3%|▊                            | 270336/9912422 [00:02<01:19, 121181.06it/s]\n",
      "\n",
      "\n",
      "\n",
      "  3%|▊                            | 294912/9912422 [00:02<01:14, 129341.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "  3%|▉                            | 311296/9912422 [00:02<01:13, 131378.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "  3%|▉                            | 335872/9912422 [00:02<01:10, 135726.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|█                            | 352256/9912422 [00:02<01:07, 141441.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|█                            | 376832/9912422 [00:03<01:07, 141500.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▏                           | 401408/9912422 [00:03<01:06, 142067.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▏                           | 417792/9912422 [00:03<01:10, 135367.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▎                           | 442368/9912422 [00:03<01:07, 140565.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▎                           | 466944/9912422 [00:03<01:04, 147057.93it/s]\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▍                           | 491520/9912422 [00:03<01:01, 152823.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▍                           | 507904/9912422 [00:04<01:08, 137792.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▌                           | 532480/9912422 [00:04<01:04, 145153.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "  6%|█▋                           | 565248/9912422 [00:04<00:58, 160486.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "  6%|█▋                           | 589824/9912422 [00:04<00:57, 162131.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "  6%|█▊                           | 614400/9912422 [00:04<00:56, 165737.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "  6%|█▊                           | 638976/9912422 [00:04<00:56, 165494.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "  7%|█▉                           | 663552/9912422 [00:04<00:55, 166621.74it/s]\n",
      "\n",
      "\n",
      "\n",
      "  7%|██                           | 696320/9912422 [00:05<00:51, 177281.10it/s]\n",
      "\n",
      "\n",
      "\n",
      "  7%|██                           | 720896/9912422 [00:05<00:50, 181137.69it/s]\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▏                          | 745472/9912422 [00:05<00:46, 196482.88it/s]\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▎                          | 770048/9912422 [00:05<00:43, 208470.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▎                          | 794624/9912422 [00:05<00:44, 204137.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▍                          | 819200/9912422 [00:05<00:46, 196015.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "  9%|██▍                          | 851968/9912422 [00:05<00:44, 202345.34it/s]\n",
      "\n",
      "\n",
      "\n",
      "  9%|██▌                          | 884736/9912422 [00:05<00:44, 205004.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "  9%|██▋                          | 917504/9912422 [00:06<00:42, 209846.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 10%|██▊                          | 950272/9912422 [00:06<00:41, 214673.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 10%|██▉                          | 983040/9912422 [00:06<00:39, 224126.23it/s]\n",
      "\n",
      "\n",
      "\n",
      " 10%|██▊                         | 1015808/9912422 [00:06<00:39, 223204.96it/s]\n",
      "\n",
      "\n",
      "\n",
      " 11%|██▉                         | 1056768/9912422 [00:06<00:37, 237298.12it/s]\n",
      "\n",
      "\n",
      "\n",
      " 11%|███                         | 1089536/9912422 [00:06<00:37, 232904.21it/s]\n",
      "\n",
      "\n",
      "\n",
      " 11%|███▏                        | 1130496/9912422 [00:07<00:35, 244044.75it/s]\n",
      "\n",
      "\n",
      "\n",
      " 12%|███▎                        | 1163264/9912422 [00:07<00:36, 237241.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 12%|███▍                        | 1204224/9912422 [00:07<00:35, 247705.18it/s]\n",
      "\n",
      "\n",
      "\n",
      " 13%|███▌                        | 1245184/9912422 [00:07<00:33, 255889.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 13%|███▋                        | 1286144/9912422 [00:07<00:32, 267585.88it/s]\n",
      "\n",
      "\n",
      "\n",
      " 13%|███▋                        | 1327104/9912422 [00:07<00:32, 265943.94it/s]\n",
      "\n",
      "\n",
      "\n",
      " 14%|███▊                        | 1359872/9912422 [00:07<00:36, 235269.09it/s]\n",
      "\n",
      "\n",
      "\n",
      " 14%|███▉                        | 1409024/9912422 [00:08<00:33, 254517.44it/s]\n",
      "\n",
      "\n",
      "\n",
      " 15%|████                        | 1449984/9912422 [00:08<00:32, 256757.48it/s]\n",
      "\n",
      "\n",
      "\n",
      " 15%|████▏                       | 1482752/9912422 [00:08<00:34, 243166.50it/s]\n",
      "\n",
      "\n",
      "\n",
      " 15%|████▎                       | 1523712/9912422 [00:08<00:33, 248054.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 16%|████▍                       | 1564672/9912422 [00:08<00:41, 200128.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 16%|████▌                       | 1630208/9912422 [00:08<00:34, 241458.01it/s]\n",
      "\n",
      "\n",
      "\n",
      " 17%|████▋                       | 1662976/9912422 [00:09<00:35, 230971.06it/s]\n",
      "\n",
      "\n",
      "\n",
      " 17%|████▊                       | 1695744/9912422 [00:09<00:35, 231580.09it/s]\n",
      "\n",
      "\n",
      "\n",
      " 17%|████▉                       | 1728512/9912422 [00:09<00:36, 223597.76it/s]\n",
      "\n",
      "\n",
      "\n",
      " 18%|████▉                       | 1753088/9912422 [00:09<00:37, 215796.00it/s]\n",
      "\n",
      "\n",
      "\n",
      " 18%|█████                       | 1794048/9912422 [00:09<00:35, 227882.71it/s]\n",
      "\n",
      "\n",
      "\n",
      " 18%|█████▏                      | 1826816/9912422 [00:09<00:36, 222125.51it/s]\n",
      "\n",
      "\n",
      "\n",
      " 19%|█████▎                      | 1859584/9912422 [00:10<00:35, 225288.04it/s]\n",
      "\n",
      "\n",
      "\n",
      " 19%|█████▎                      | 1884160/9912422 [00:10<00:43, 185304.33it/s]\n",
      "\n",
      "\n",
      "\n",
      " 19%|█████▍                      | 1925120/9912422 [00:10<00:38, 206462.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 20%|█████▌                      | 1949696/9912422 [00:10<00:41, 193145.76it/s]\n",
      "\n",
      "\n",
      "\n",
      " 20%|█████▌                      | 1974272/9912422 [00:10<00:42, 185180.51it/s]\n",
      "\n",
      "\n",
      "\n",
      " 20%|█████▋                      | 1998848/9912422 [00:10<00:40, 195764.49it/s]\n",
      "\n",
      "\n",
      "\n",
      " 20%|█████▋                      | 2031616/9912422 [00:10<00:41, 188933.59it/s]\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████▊                      | 2064384/9912422 [00:11<00:39, 200301.21it/s]\n",
      "\n",
      "\n",
      "\n",
      " 21%|█████▉                      | 2097152/9912422 [00:11<00:38, 203034.82it/s]\n",
      "\n",
      "\n",
      "\n",
      " 21%|██████                      | 2129920/9912422 [00:11<00:37, 207993.50it/s]\n",
      "\n",
      "\n",
      "\n",
      " 22%|██████                      | 2162688/9912422 [00:11<00:36, 212454.56it/s]\n",
      "\n",
      "\n",
      "\n",
      " 22%|██████▏                     | 2195456/9912422 [00:11<00:33, 227856.89it/s]\n",
      "\n",
      "\n",
      "\n",
      " 22%|██████▎                     | 2228224/9912422 [00:11<00:33, 229161.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 23%|██████▍                     | 2269184/9912422 [00:11<00:31, 241558.11it/s]\n",
      "\n",
      "\n",
      "\n",
      " 23%|██████▌                     | 2301952/9912422 [00:12<00:31, 238905.29it/s]\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████▌                     | 2334720/9912422 [00:12<00:32, 233942.65it/s]\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████▋                     | 2359296/9912422 [00:12<00:41, 182152.28it/s]\n",
      "\n",
      "\n",
      "\n",
      " 24%|██████▊                     | 2392064/9912422 [00:12<00:45, 165058.24it/s]\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████▊                     | 2433024/9912422 [00:12<00:39, 189720.33it/s]\n",
      "\n",
      "\n",
      "\n",
      " 25%|██████▉                     | 2457600/9912422 [00:13<00:48, 152655.86it/s]\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████                     | 2482176/9912422 [00:13<00:48, 154015.38it/s]\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████                     | 2506752/9912422 [00:13<00:52, 139895.73it/s]\n",
      "\n",
      "\n",
      "\n",
      " 25%|███████▏                    | 2523136/9912422 [00:13<00:56, 130489.03it/s]\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████▏                    | 2547712/9912422 [00:13<00:56, 129562.34it/s]\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████▎                    | 2572288/9912422 [00:13<00:53, 138008.13it/s]\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████▎                    | 2588672/9912422 [00:14<00:56, 130632.52it/s]\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████▍                    | 2613248/9912422 [00:14<00:51, 141330.03it/s]\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████▋                     | 2629632/9912422 [00:14<01:16, 94989.54it/s]\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████▌                    | 2662400/9912422 [00:14<01:00, 120354.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████▌                    | 2686976/9912422 [00:14<00:58, 123405.22it/s]\n",
      "\n",
      "\n",
      "\n",
      " 27%|███████▋                    | 2711552/9912422 [00:14<00:57, 125608.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████▋                    | 2727936/9912422 [00:15<00:54, 132002.96it/s]\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████▊                    | 2744320/9912422 [00:15<00:56, 126938.05it/s]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████▊                    | 2760704/9912422 [00:15<01:00, 118454.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████▊                    | 2785280/9912422 [00:15<00:54, 129716.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 28%|███████▉                    | 2801664/9912422 [00:15<00:57, 124181.58it/s]\n",
      "\n",
      "\n",
      "\n",
      " 29%|███████▉                    | 2826240/9912422 [00:15<00:52, 135962.32it/s]\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████                    | 2850816/9912422 [00:15<00:49, 141726.34it/s]\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████                    | 2875392/9912422 [00:16<00:46, 150246.28it/s]\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████▏                   | 2899968/9912422 [00:16<00:46, 152290.90it/s]\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████▏                   | 2916352/9912422 [00:16<00:50, 137931.99it/s]\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████▎                   | 2940928/9912422 [00:16<00:48, 145217.03it/s]\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████▍                   | 2965504/9912422 [00:16<00:45, 151424.86it/s]\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████▍                   | 2990080/9912422 [00:16<00:43, 158366.27it/s]\n",
      "\n",
      "\n",
      "\n",
      " 30%|████████▌                   | 3014656/9912422 [00:16<00:43, 158047.29it/s]\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████▌                   | 3031040/9912422 [00:17<00:52, 131993.87it/s]\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████▋                   | 3063808/9912422 [00:17<00:44, 155082.96it/s]\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████▋                   | 3088384/9912422 [00:17<00:44, 151855.91it/s]\n",
      "\n",
      "\n",
      "\n",
      " 31%|████████▊                   | 3112960/9912422 [00:17<00:49, 136279.50it/s]\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████▊                   | 3129344/9912422 [00:17<00:58, 115212.70it/s]\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████▉                   | 3153920/9912422 [00:17<00:49, 135552.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 32%|████████▉                   | 3170304/9912422 [00:18<00:53, 127042.16it/s]\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████                   | 3186688/9912422 [00:18<00:55, 121659.23it/s]\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████                   | 3203072/9912422 [00:18<00:56, 118243.33it/s]\n",
      "\n",
      "\n",
      "\n",
      " 32%|█████████▍                   | 3219456/9912422 [00:18<01:14, 89856.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████▏                  | 3252224/9912422 [00:18<01:05, 102114.10it/s]\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████▏                  | 3268608/9912422 [00:19<01:02, 106044.37it/s]\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████▎                  | 3284992/9912422 [00:19<01:02, 105695.87it/s]\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████▎                  | 3301376/9912422 [00:19<01:02, 106454.01it/s]\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████▎                  | 3317760/9912422 [00:19<01:01, 107318.44it/s]\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████▍                  | 3334144/9912422 [00:19<01:00, 108832.42it/s]\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████▍                  | 3350528/9912422 [00:19<00:58, 112973.24it/s]\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████▌                  | 3375104/9912422 [00:19<00:52, 123402.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████▌                  | 3391488/9912422 [00:20<00:53, 121249.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 34%|█████████▋                  | 3407872/9912422 [00:20<00:54, 119786.92it/s]\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████▋                  | 3424256/9912422 [00:20<00:49, 129780.73it/s]\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████▋                  | 3440640/9912422 [00:20<00:51, 125077.74it/s]\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████▊                  | 3457024/9912422 [00:20<00:50, 128234.99it/s]\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████▊                  | 3473408/9912422 [00:20<00:47, 135779.65it/s]\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████▊                  | 3489792/9912422 [00:20<00:49, 128623.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 35%|█████████▉                  | 3514368/9912422 [00:20<00:45, 139677.66it/s]\n",
      "\n",
      "\n",
      "\n",
      " 36%|█████████▉                  | 3530752/9912422 [00:21<00:48, 131812.96it/s]\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████                  | 3555328/9912422 [00:21<00:46, 137456.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████                  | 3571712/9912422 [00:21<00:48, 131509.49it/s]\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████▏                 | 3588096/9912422 [00:21<01:00, 104639.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████▏                 | 3620864/9912422 [00:21<00:50, 125356.66it/s]\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████▎                 | 3645440/9912422 [00:21<00:46, 133491.51it/s]\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████▎                 | 3661824/9912422 [00:22<00:48, 127881.72it/s]\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████▍                 | 3678208/9912422 [00:22<00:50, 124253.74it/s]\n",
      "\n",
      "\n",
      "\n",
      " 37%|██████████▍                 | 3702784/9912422 [00:22<00:47, 131937.09it/s]\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████▌                 | 3727360/9912422 [00:22<00:44, 140349.05it/s]\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████▌                 | 3743744/9912422 [00:22<00:54, 113760.08it/s]\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████▋                 | 3776512/9912422 [00:22<00:46, 131859.54it/s]\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████▋                 | 3792896/9912422 [00:23<00:48, 126860.21it/s]\n",
      "\n",
      "\n",
      "\n",
      " 38%|██████████▊                 | 3809280/9912422 [00:23<00:51, 119340.28it/s]\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████▊                 | 3825664/9912422 [00:23<00:52, 116907.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████▊                 | 3842048/9912422 [00:23<00:52, 115197.87it/s]\n",
      "\n",
      "\n",
      "\n",
      " 39%|██████████▉                 | 3858432/9912422 [00:23<00:51, 118039.15it/s]\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████▎                 | 3874816/9912422 [00:23<01:01, 98433.30it/s]\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████                 | 3899392/9912422 [00:24<00:54, 110875.87it/s]\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████▍                 | 3915776/9912422 [00:24<01:00, 99683.52it/s]\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████▌                 | 3932160/9912422 [00:24<01:00, 98719.17it/s]\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████▏                | 3948544/9912422 [00:24<00:58, 102016.54it/s]\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████▏                | 3964928/9912422 [00:24<00:56, 104780.09it/s]\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████▎                | 3989504/9912422 [00:24<00:50, 118170.75it/s]\n",
      "\n",
      "\n",
      "\n",
      " 40%|███████████▎                | 4005888/9912422 [00:24<00:50, 117666.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████▎                | 4022272/9912422 [00:25<00:50, 117316.46it/s]\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████▍                | 4046848/9912422 [00:25<00:46, 126998.01it/s]\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████▍                | 4063232/9912422 [00:25<00:48, 119879.67it/s]\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████▌                | 4079616/9912422 [00:25<00:49, 118831.78it/s]\n",
      "\n",
      "\n",
      "\n",
      " 41%|███████████▌                | 4104192/9912422 [00:25<00:45, 128229.40it/s]\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████▋                | 4120576/9912422 [00:25<00:46, 124485.94it/s]\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████▋                | 4136960/9912422 [00:26<00:47, 121966.85it/s]\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████▊                | 4161536/9912422 [00:26<00:43, 130776.55it/s]\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████▊                | 4186112/9912422 [00:26<00:41, 139144.22it/s]\n",
      "\n",
      "\n",
      "\n",
      " 42%|███████████▊                | 4202496/9912422 [00:26<00:43, 130557.57it/s]\n",
      "\n",
      "\n",
      "\n",
      " 43%|███████████▉                | 4227072/9912422 [00:26<00:40, 139742.50it/s]\n",
      "\n",
      "\n",
      "\n",
      " 43%|███████████▉                | 4243456/9912422 [00:26<00:42, 132639.24it/s]\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████                | 4259840/9912422 [00:26<00:46, 122879.48it/s]\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████                | 4284416/9912422 [00:27<00:41, 134887.63it/s]\n",
      "\n",
      "\n",
      "\n",
      " 43%|████████████▏               | 4300800/9912422 [00:27<00:43, 128791.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████▏               | 4325376/9912422 [00:27<00:46, 120962.04it/s]\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████▎               | 4349952/9912422 [00:27<00:42, 131777.77it/s]\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████▎               | 4374528/9912422 [00:27<00:39, 140491.37it/s]\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████▍               | 4390912/9912422 [00:27<00:42, 131154.24it/s]\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████▍               | 4415488/9912422 [00:27<00:38, 141753.22it/s]\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████▌               | 4440064/9912422 [00:28<00:36, 150267.62it/s]\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████▌               | 4456448/9912422 [00:28<00:41, 132988.79it/s]\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████▋               | 4481024/9912422 [00:28<00:38, 139785.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 45%|████████████▋               | 4497408/9912422 [00:28<00:41, 129396.85it/s]\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████▊               | 4513792/9912422 [00:28<00:42, 127005.63it/s]\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████▊               | 4538368/9912422 [00:28<00:39, 134777.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████▉               | 4562944/9912422 [00:29<00:39, 137139.40it/s]\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████▉               | 4579328/9912422 [00:29<00:40, 130221.11it/s]\n",
      "\n",
      "\n",
      "\n",
      " 46%|████████████▉               | 4595712/9912422 [00:29<00:49, 106865.94it/s]\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████               | 4628480/9912422 [00:29<00:40, 129427.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████               | 4644864/9912422 [00:29<00:42, 123600.75it/s]\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████▏              | 4661248/9912422 [00:29<00:42, 123326.42it/s]\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████▏              | 4677632/9912422 [00:29<00:43, 121185.15it/s]\n",
      "\n",
      "\n",
      "\n",
      " 47%|█████████████▎              | 4694016/9912422 [00:30<00:45, 115776.23it/s]\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████▎              | 4710400/9912422 [00:30<00:44, 115994.57it/s]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████▎              | 4726784/9912422 [00:30<00:43, 119028.28it/s]\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████▍              | 4743168/9912422 [00:30<00:44, 116739.44it/s]\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████▍              | 4767744/9912422 [00:30<00:42, 119864.18it/s]\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████▌              | 4792320/9912422 [00:30<00:39, 129076.97it/s]\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████▌              | 4808704/9912422 [00:31<00:39, 129668.56it/s]\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████▋              | 4825088/9912422 [00:31<00:37, 135114.59it/s]\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████▋              | 4849664/9912422 [00:31<00:35, 141080.40it/s]\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████▋              | 4866048/9912422 [00:31<00:34, 144332.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 49%|█████████████▊              | 4890624/9912422 [00:31<00:34, 143705.15it/s]\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████▉              | 4915200/9912422 [00:31<00:33, 147526.45it/s]\n",
      "\n",
      "\n",
      "\n",
      " 50%|█████████████▉              | 4939776/9912422 [00:31<00:32, 154769.26it/s]\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████              | 4964352/9912422 [00:32<00:31, 155507.73it/s]\n",
      "\n",
      "\n",
      "\n",
      " 50%|██████████████              | 4988928/9912422 [00:32<00:30, 160822.95it/s]\n",
      "\n",
      "\n",
      "\n",
      " 51%|██████████████▏             | 5013504/9912422 [00:32<00:30, 158917.53it/s]\n",
      "\n",
      "\n",
      "\n",
      " 51%|██████████████▏             | 5038080/9912422 [00:32<00:27, 176109.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 51%|██████████████▎             | 5062656/9912422 [00:32<00:27, 173315.08it/s]\n",
      "\n",
      "\n",
      "\n",
      " 51%|██████████████▎             | 5087232/9912422 [00:32<00:26, 184636.87it/s]\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████▍             | 5111808/9912422 [00:32<00:26, 184341.16it/s]\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████▌             | 5136384/9912422 [00:32<00:25, 187856.91it/s]\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████▌             | 5160960/9912422 [00:33<00:24, 197573.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 52%|██████████████▋             | 5185536/9912422 [00:33<00:23, 204995.89it/s]\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████▋             | 5210112/9912422 [00:33<00:24, 194881.10it/s]\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████▊             | 5234688/9912422 [00:33<00:23, 201151.76it/s]\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████▊             | 5259264/9912422 [00:33<00:23, 195574.04it/s]\n",
      "\n",
      "\n",
      "\n",
      " 53%|██████████████▉             | 5283840/9912422 [00:33<00:22, 205092.95it/s]\n",
      "\n",
      "\n",
      "\n",
      " 54%|███████████████             | 5316608/9912422 [00:33<00:19, 230940.87it/s]\n",
      "\n",
      "\n",
      "\n",
      " 54%|███████████████             | 5349376/9912422 [00:33<00:19, 231561.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 54%|███████████████▏            | 5373952/9912422 [00:33<00:19, 229471.72it/s]\n",
      "\n",
      "\n",
      "\n",
      " 54%|███████████████▏            | 5398528/9912422 [00:34<00:20, 218499.23it/s]\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████▎            | 5423104/9912422 [00:34<00:20, 220320.62it/s]\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████▍            | 5455872/9912422 [00:34<00:18, 240151.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 55%|███████████████▌            | 5488640/9912422 [00:34<00:18, 234900.67it/s]\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████▌            | 5513216/9912422 [00:34<00:18, 236568.67it/s]\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████▋            | 5545984/9912422 [00:34<00:18, 240176.04it/s]\n",
      "\n",
      "\n",
      "\n",
      " 56%|███████████████▊            | 5578752/9912422 [00:34<00:16, 257969.84it/s]\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████▊            | 5619712/9912422 [00:34<00:16, 267130.53it/s]\n",
      "\n",
      "\n",
      "\n",
      " 57%|███████████████▉            | 5652480/9912422 [00:35<00:15, 276106.32it/s]\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████            | 5701632/9912422 [00:35<00:14, 294674.60it/s]\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████▏           | 5734400/9912422 [00:35<00:14, 294552.44it/s]\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████▎           | 5767168/9912422 [00:35<00:14, 280664.10it/s]\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████▍           | 5808128/9912422 [00:35<00:13, 306222.41it/s]\n",
      "\n",
      "\n",
      "\n",
      " 59%|████████████████▍           | 5840896/9912422 [00:35<00:16, 242655.41it/s]\n",
      "\n",
      "\n",
      "\n",
      " 60%|████████████████▊           | 5931008/9912422 [00:35<00:13, 302947.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 60%|████████████████▊           | 5971968/9912422 [00:36<00:17, 222963.67it/s]\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████           | 6037504/9912422 [00:36<00:14, 274709.23it/s]\n",
      "\n",
      "\n",
      "\n",
      " 61%|█████████████████▏          | 6078464/9912422 [00:36<00:19, 197619.56it/s]\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████▎          | 6127616/9912422 [00:36<00:16, 226795.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████▍          | 6168576/9912422 [00:37<00:18, 204238.02it/s]\n",
      "\n",
      "\n",
      "\n",
      " 63%|█████████████████▌          | 6201344/9912422 [00:37<00:18, 205850.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 63%|█████████████████▌          | 6234112/9912422 [00:37<00:19, 193475.13it/s]\n",
      "\n",
      "\n",
      "\n",
      " 63%|█████████████████▋          | 6258688/9912422 [00:37<00:19, 185698.63it/s]\n",
      "\n",
      "\n",
      "\n",
      " 63%|█████████████████▋          | 6283264/9912422 [00:37<00:18, 196975.96it/s]\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████▊          | 6307840/9912422 [00:37<00:18, 191980.40it/s]\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████▉          | 6332416/9912422 [00:37<00:17, 200760.42it/s]\n",
      "\n",
      "\n",
      "\n",
      " 64%|█████████████████▉          | 6356992/9912422 [00:38<00:18, 192171.49it/s]\n",
      "\n",
      "\n",
      "\n",
      " 64%|██████████████████          | 6381568/9912422 [00:38<00:17, 200894.56it/s]\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████          | 6406144/9912422 [00:38<00:17, 199588.51it/s]\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████▏         | 6430720/9912422 [00:38<00:18, 184418.13it/s]\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████▏         | 6455296/9912422 [00:38<00:17, 195771.70it/s]\n",
      "\n",
      "\n",
      "\n",
      " 65%|██████████████████▎         | 6479872/9912422 [00:38<00:16, 206526.10it/s]\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████▎         | 6504448/9912422 [00:38<00:15, 213408.86it/s]\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████▍         | 6529024/9912422 [00:38<00:16, 210967.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████▌         | 6553600/9912422 [00:38<00:15, 214905.54it/s]\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████▌         | 6586368/9912422 [00:39<00:15, 220035.42it/s]\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████▋         | 6610944/9912422 [00:39<00:14, 221412.12it/s]\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████▊         | 6643712/9912422 [00:39<00:14, 219765.75it/s]\n",
      "\n",
      "\n",
      "\n",
      " 67%|██████████████████▊         | 6668288/9912422 [00:39<00:14, 226791.45it/s]\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████▉         | 6692864/9912422 [00:39<00:15, 213915.07it/s]\n",
      "\n",
      "\n",
      "\n",
      " 68%|██████████████████▉         | 6725632/9912422 [00:39<00:13, 237217.68it/s]\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████         | 6758400/9912422 [00:39<00:13, 236334.15it/s]\n",
      "\n",
      "\n",
      "\n",
      " 68%|███████████████████▏        | 6782976/9912422 [00:39<00:13, 232730.45it/s]\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████▎        | 6815744/9912422 [00:40<00:12, 249448.77it/s]\n",
      "\n",
      "\n",
      "\n",
      " 69%|███████████████████▎        | 6848512/9912422 [00:40<00:15, 201919.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████▍        | 6889472/9912422 [00:40<00:12, 235064.47it/s]\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████▌        | 6922240/9912422 [00:40<00:13, 215561.22it/s]\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████▌        | 6946816/9912422 [00:40<00:13, 216150.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████▋        | 6971392/9912422 [00:40<00:14, 199378.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████▊        | 7004160/9912422 [00:40<00:13, 208403.69it/s]\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████▉        | 7036928/9912422 [00:41<00:13, 208794.28it/s]\n",
      "\n",
      "\n",
      "\n",
      " 71%|███████████████████▉        | 7077888/9912422 [00:41<00:12, 228177.45it/s]\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████        | 7110656/9912422 [00:41<00:12, 223530.40it/s]\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████▏       | 7151616/9912422 [00:41<00:11, 237068.52it/s]\n",
      "\n",
      "\n",
      "\n",
      " 72%|████████████████████▎       | 7184384/9912422 [00:41<00:11, 236806.11it/s]\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████▍       | 7225344/9912422 [00:41<00:10, 250900.54it/s]\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████▌       | 7258112/9912422 [00:41<00:10, 263730.23it/s]\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████▌       | 7290880/9912422 [00:42<00:09, 263251.59it/s]\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████▋       | 7331840/9912422 [00:42<00:09, 262917.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 74%|████████████████████▊       | 7372800/9912422 [00:42<00:09, 265345.63it/s]\n",
      "\n",
      "\n",
      "\n",
      " 75%|████████████████████▉       | 7413760/9912422 [00:42<00:09, 269233.92it/s]\n",
      "\n",
      "\n",
      "\n",
      " 75%|█████████████████████       | 7446528/9912422 [00:42<00:09, 260220.52it/s]\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████▏      | 7487488/9912422 [00:42<00:09, 260811.70it/s]\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████▎      | 7528448/9912422 [00:42<00:08, 269238.82it/s]\n",
      "\n",
      "\n",
      "\n",
      " 76%|█████████████████████▍      | 7569408/9912422 [00:43<00:08, 267091.93it/s]\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████▍      | 7610368/9912422 [00:43<00:08, 273888.78it/s]\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████▌      | 7643136/9912422 [00:43<00:07, 286514.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 77%|█████████████████████▋      | 7675904/9912422 [00:43<00:08, 267301.95it/s]\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████▊      | 7708672/9912422 [00:43<00:08, 265802.14it/s]\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████▊      | 7741440/9912422 [00:43<00:08, 260661.63it/s]\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████▉      | 7782400/9912422 [00:43<00:08, 261130.18it/s]\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████      | 7831552/9912422 [00:44<00:07, 282541.99it/s]\n",
      "\n",
      "\n",
      "\n",
      " 79%|██████████████████████▏     | 7864320/9912422 [00:44<00:09, 216295.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████▍     | 7929856/9912422 [00:44<00:09, 214018.37it/s]\n",
      "\n",
      "\n",
      "\n",
      " 80%|██████████████████████▌     | 7979008/9912422 [00:44<00:07, 248562.28it/s]\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████▋     | 8011776/9912422 [00:45<00:10, 185471.04it/s]\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████▋     | 8044544/9912422 [00:45<00:08, 209400.27it/s]\n",
      "\n",
      "\n",
      "\n",
      " 81%|██████████████████████▊     | 8077312/9912422 [00:45<00:09, 190304.30it/s]\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████▉     | 8110080/9912422 [00:45<00:09, 198976.24it/s]\n",
      "\n",
      "\n",
      "\n",
      " 82%|██████████████████████▉     | 8134656/9912422 [00:45<00:09, 188472.13it/s]\n",
      "\n",
      "\n",
      "\n",
      " 82%|███████████████████████     | 8159232/9912422 [00:45<00:09, 185104.05it/s]\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████     | 8183808/9912422 [00:45<00:09, 175795.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████▏    | 8216576/9912422 [00:46<00:08, 189774.31it/s]\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████▎    | 8241152/9912422 [00:46<00:09, 184987.51it/s]\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████▎    | 8265728/9912422 [00:46<00:08, 193143.35it/s]\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████▍    | 8290304/9912422 [00:46<00:08, 184634.96it/s]\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████▍    | 8314880/9912422 [00:46<00:08, 185632.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████▌    | 8347648/9912422 [00:46<00:08, 192612.53it/s]\n",
      "\n",
      "\n",
      "\n",
      " 84%|███████████████████████▋    | 8372224/9912422 [00:46<00:07, 195826.95it/s]\n",
      "\n",
      "\n",
      "\n",
      " 85%|███████████████████████▋    | 8404992/9912422 [00:47<00:07, 199795.58it/s]\n",
      "\n",
      "\n",
      "\n",
      " 85%|███████████████████████▊    | 8437760/9912422 [00:47<00:07, 208722.22it/s]\n",
      "\n",
      "\n",
      "\n",
      " 85%|███████████████████████▉    | 8470528/9912422 [00:47<00:06, 206820.15it/s]\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████    | 8511488/9912422 [00:47<00:06, 212540.64it/s]\n",
      "\n",
      "\n",
      "\n",
      " 86%|████████████████████████▏   | 8544256/9912422 [00:47<00:05, 228625.84it/s]\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████▏   | 8577024/9912422 [00:47<00:05, 230667.63it/s]\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████▎   | 8609792/9912422 [00:47<00:05, 231366.02it/s]\n",
      "\n",
      "\n",
      "\n",
      " 87%|████████████████████████▍   | 8650752/9912422 [00:48<00:05, 233406.56it/s]\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████▌   | 8675328/9912422 [00:48<00:07, 169543.11it/s]\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████▌   | 8716288/9912422 [00:48<00:06, 189436.81it/s]\n",
      "\n",
      "\n",
      "\n",
      " 88%|████████████████████████▋   | 8740864/9912422 [00:48<00:06, 179196.78it/s]\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████████████████████▊   | 8773632/9912422 [00:48<00:06, 187358.09it/s]\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████████████████████▊   | 8798208/9912422 [00:48<00:05, 190039.84it/s]\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████████████████████▉   | 8830976/9912422 [00:49<00:05, 195542.82it/s]\n",
      "\n",
      "\n",
      "\n",
      " 89%|█████████████████████████   | 8863744/9912422 [00:49<00:05, 205454.74it/s]\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████▏  | 8896512/9912422 [00:49<00:04, 229192.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████▏  | 8921088/9912422 [00:49<00:04, 224399.67it/s]\n",
      "\n",
      "\n",
      "\n",
      " 90%|█████████████████████████▎  | 8945664/9912422 [00:49<00:04, 208688.14it/s]\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████████████████████▎  | 8978432/9912422 [00:49<00:04, 215417.05it/s]\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████████████████████▍  | 9011200/9912422 [00:49<00:04, 213684.27it/s]\n",
      "\n",
      "\n",
      "\n",
      " 91%|█████████████████████████▌  | 9052160/9912422 [00:50<00:03, 226218.98it/s]\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████████████████████▋  | 9093120/9912422 [00:50<00:03, 223832.10it/s]\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████████████████████▊  | 9125888/9912422 [00:50<00:03, 221000.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 92%|█████████████████████████▉  | 9166848/9912422 [00:50<00:03, 235579.16it/s]\n",
      "\n",
      "\n",
      "\n",
      " 93%|██████████████████████████  | 9207808/9912422 [00:50<00:02, 238044.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 93%|██████████████████████████▏ | 9248768/9912422 [00:50<00:02, 251540.50it/s]\n",
      "\n",
      "\n",
      "\n",
      " 94%|██████████████████████████▏ | 9281536/9912422 [00:50<00:02, 254629.53it/s]\n",
      "\n",
      "\n",
      "\n",
      " 94%|██████████████████████████▎ | 9322496/9912422 [00:51<00:02, 249503.73it/s]\n",
      "\n",
      "\n",
      "\n",
      " 94%|██████████████████████████▍ | 9355264/9912422 [00:51<00:02, 208538.38it/s]\n",
      "\n",
      "\n",
      "\n",
      " 95%|██████████████████████████▌ | 9404416/9912422 [00:51<00:02, 250844.03it/s]\n",
      "\n",
      "\n",
      "\n",
      " 95%|██████████████████████████▋ | 9437184/9912422 [00:51<00:02, 219195.29it/s]\n",
      "\n",
      "\n",
      "\n",
      " 96%|██████████████████████████▊ | 9469952/9912422 [00:51<00:01, 222210.25it/s]\n",
      "\n",
      "\n",
      "\n",
      " 96%|██████████████████████████▊ | 9502720/9912422 [00:51<00:01, 218326.77it/s]\n",
      "\n",
      "\n",
      "\n",
      " 96%|██████████████████████████▉ | 9527296/9912422 [00:52<00:01, 203124.43it/s]\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████ | 9568256/9912422 [00:52<00:02, 171549.73it/s]\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████▏| 9625600/9912422 [00:52<00:01, 205457.53it/s]\n",
      "\n",
      "\n",
      "\n",
      " 97%|███████████████████████████▎| 9658368/9912422 [00:52<00:01, 192366.64it/s]\n",
      "\n",
      "\n",
      "\n",
      " 98%|███████████████████████████▎| 9682944/9912422 [00:52<00:01, 201015.34it/s]\n",
      "\n",
      "\n",
      "\n",
      " 98%|███████████████████████████▍| 9707520/9912422 [00:52<00:01, 185539.32it/s]\n",
      "\n",
      "\n",
      "\n",
      " 98%|███████████████████████████▍| 9732096/9912422 [00:53<00:01, 170348.17it/s]\n",
      "\n",
      "\n",
      "\n",
      " 99%|███████████████████████████▌| 9764864/9912422 [00:53<00:00, 182901.83it/s]\n",
      "\n",
      "\n",
      "\n",
      " 99%|███████████████████████████▋| 9789440/9912422 [00:53<00:00, 169083.19it/s]\n",
      "\n",
      "\n",
      "\n",
      " 99%|███████████████████████████▋| 9822208/9912422 [00:53<00:00, 181258.84it/s]\n",
      "\n",
      "\n",
      "\n",
      " 99%|███████████████████████████▊| 9854976/9912422 [00:53<00:00, 189251.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████▉| 9879552/9912422 [00:53<00:00, 184658.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|███████████████████████████▉| 9912320/9912422 [00:54<00:00, 191522.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /files/MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /files/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                | 0/28881 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 57%|██████████████████▋              | 16384/28881 [00:00<00:00, 53151.63it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "32768it [00:00, 48281.79it/s]                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /files/MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /files/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                              | 0/1648877 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  1%|▎                              | 16384/1648877 [00:00<00:19, 85594.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  2%|▌                              | 32768/1648877 [00:00<00:17, 90385.90it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  3%|▉                              | 49152/1648877 [00:00<00:16, 95672.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  4%|█▏                             | 65536/1648877 [00:00<00:16, 98255.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  5%|█▋                            | 90112/1648877 [00:01<00:13, 113109.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  6%|█▊                           | 106496/1648877 [00:01<00:13, 110500.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  8%|██▎                          | 131072/1648877 [00:01<00:12, 121326.92it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  9%|██▌                          | 147456/1648877 [00:01<00:12, 116113.51it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 10%|███                          | 172032/1648877 [00:01<00:11, 123376.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 12%|███▍                         | 196608/1648877 [00:01<00:11, 131907.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 13%|███▉                         | 221184/1648877 [00:02<00:10, 142381.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 15%|████▎                        | 245760/1648877 [00:02<00:09, 150761.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 17%|████▉                        | 278528/1648877 [00:02<00:08, 153780.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 19%|█████▍                       | 311296/1648877 [00:02<00:08, 165696.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 20%|█████▉                       | 335872/1648877 [00:02<00:07, 165532.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 22%|██████▍                      | 368640/1648877 [00:02<00:07, 179236.22it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 24%|███████                      | 401408/1648877 [00:03<00:06, 194145.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 26%|███████▋                     | 434176/1648877 [00:03<00:06, 198567.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 29%|████████▎                    | 475136/1648877 [00:03<00:05, 214147.86it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 31%|█████████                    | 516096/1648877 [00:03<00:04, 232625.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 33%|█████████▋                   | 548864/1648877 [00:03<00:04, 228941.19it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 36%|██████████▎                  | 589824/1648877 [00:03<00:04, 252432.41it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 39%|███████████▏                 | 638976/1648877 [00:03<00:03, 261499.94it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 42%|████████████                 | 688128/1648877 [00:04<00:03, 285653.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 44%|████████████▊                | 729088/1648877 [00:04<00:03, 278168.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 48%|█████████████▊               | 786432/1648877 [00:04<00:02, 307487.72it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 51%|██████████████▋              | 835584/1648877 [00:04<00:02, 318995.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 54%|███████████████▋             | 892928/1648877 [00:04<00:02, 329128.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 58%|████████████████▋            | 950272/1648877 [00:04<00:02, 346120.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 62%|█████████████████▏          | 1015808/1648877 [00:04<00:01, 372136.73it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 66%|██████████████████▎         | 1081344/1648877 [00:05<00:01, 385182.27it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 70%|███████████████████▍        | 1146880/1648877 [00:05<00:01, 418470.87it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 73%|████████████████████▍       | 1204224/1648877 [00:05<00:01, 429791.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 78%|█████████████████████▊      | 1286144/1648877 [00:05<00:00, 454358.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 83%|███████████████████████▏    | 1368064/1648877 [00:05<00:00, 494644.95it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 89%|████████████████████████▉   | 1466368/1648877 [00:05<00:00, 517502.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 95%|██████████████████████████▌ | 1564672/1648877 [00:05<00:00, 576911.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1654784it [00:05, 276346.19it/s]                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /files/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /files/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|                                                 | 0/4542 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /files/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaya\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\Gaya\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:46: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9920512it [01:13, 191522.42it/s]                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3004, Accuracy: 751/10000 (7%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.268457\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.268337\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.274000\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.251669\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.236826\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.223579\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.153440\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.096377\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 1.974148\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 1.944884\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.839482\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 1.764007\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 1.698028\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 1.690848\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 1.507314\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 1.391876\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 1.283325\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.328951\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.280170\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.239683\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.917854\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 1.397346\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.953183\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 1.098235\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.953593\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.039020\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.943539\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.808112\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 1.122505\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.652426\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.934654\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.782686\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.803863\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.757899\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.945375\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.622024\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.861092\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.635435\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.804188\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.820268\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.761757\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.732126\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.756373\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.737989\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.776553\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.705495\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.660016\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 1.021392\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.665041\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.885761\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.717163\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.686083\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.573556\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.468483\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.771595\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.464924\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.759291\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.335701\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.381030\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.578340\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.721811\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.434272\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.492975\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.576578\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.457780\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.390356\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.600620\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.605990\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.588093\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.483013\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.592734\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.722786\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.572126\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.876553\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.383251\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.458787\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.345329\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.594702\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.364994\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.390614\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.781098\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.459865\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.639604\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.765076\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.551426\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.450237\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.727741\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.482011\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.528552\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.555832\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.340482\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.480175\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.335776\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.487062\n",
      "\n",
      "Test set: Avg. loss: 0.1975, Accuracy: 9437/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.549442\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.314246\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.381319\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.284227\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.468293\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.670883\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.527117\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.457847\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.384555\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.474007\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.523543\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.414188\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.310911\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.339576\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.404720\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.398672\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.390989\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.588507\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.572979\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.507679\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.393849\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.490485\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.365966\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.328282\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.563094\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.390059\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.510995\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.388239\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.347582\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.374173\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.479837\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.333436\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.391494\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.705925\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.401420\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.421048\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.450646\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.427474\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.330066\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.348021\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.467912\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.334478\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.427918\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.377581\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.299903\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.445684\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.426756\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.643597\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.343843\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.398843\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.438476\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.340536\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.272500\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.371476\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.417185\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.636687\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.253277\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.489093\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.409328\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.358625\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.213364\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.312311\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.369296\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.373436\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.336469\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.298777\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.181575\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.222686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.366992\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.373561\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.466382\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.447926\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.434536\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.268989\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.290256\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.485988\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.358059\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.365614\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.315572\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.307521\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.433961\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.193908\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.394036\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.402119\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.288014\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.330749\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.356706\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.632195\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.332151\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.562656\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.384818\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.270625\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.205928\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.345963\n",
      "\n",
      "Test set: Avg. loss: 0.1264, Accuracy: 9614/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.272734\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.312503\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.392842\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.264086\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.379864\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.472037\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.394774\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.158641\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.240880\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.427967\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.350290\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.278986\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.379178\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.248193\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.389152\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.356425\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.440390\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.321186\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.272470\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.458673\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.291443\n",
      "Train Epoch: 3 [13440/60000 (22%)]\tLoss: 0.215607\n",
      "Train Epoch: 3 [14080/60000 (23%)]\tLoss: 0.202505\n",
      "Train Epoch: 3 [14720/60000 (25%)]\tLoss: 0.310053\n",
      "Train Epoch: 3 [15360/60000 (26%)]\tLoss: 0.198976\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.423587\n",
      "Train Epoch: 3 [16640/60000 (28%)]\tLoss: 0.266243\n",
      "Train Epoch: 3 [17280/60000 (29%)]\tLoss: 0.489140\n",
      "Train Epoch: 3 [17920/60000 (30%)]\tLoss: 0.263369\n",
      "Train Epoch: 3 [18560/60000 (31%)]\tLoss: 0.166505\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.581683\n",
      "Train Epoch: 3 [19840/60000 (33%)]\tLoss: 0.224020\n",
      "Train Epoch: 3 [20480/60000 (34%)]\tLoss: 0.343955\n",
      "Train Epoch: 3 [21120/60000 (35%)]\tLoss: 0.362808\n",
      "Train Epoch: 3 [21760/60000 (36%)]\tLoss: 0.293537\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.337888\n",
      "Train Epoch: 3 [23040/60000 (38%)]\tLoss: 0.415865\n",
      "Train Epoch: 3 [23680/60000 (39%)]\tLoss: 0.304252\n",
      "Train Epoch: 3 [24320/60000 (41%)]\tLoss: 0.318476\n",
      "Train Epoch: 3 [24960/60000 (42%)]\tLoss: 0.141127\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.295053\n",
      "Train Epoch: 3 [26240/60000 (44%)]\tLoss: 0.453026\n",
      "Train Epoch: 3 [26880/60000 (45%)]\tLoss: 0.250097\n",
      "Train Epoch: 3 [27520/60000 (46%)]\tLoss: 0.382482\n",
      "Train Epoch: 3 [28160/60000 (47%)]\tLoss: 0.231856\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.148784\n",
      "Train Epoch: 3 [29440/60000 (49%)]\tLoss: 0.185288\n",
      "Train Epoch: 3 [30080/60000 (50%)]\tLoss: 0.211943\n",
      "Train Epoch: 3 [30720/60000 (51%)]\tLoss: 0.326149\n",
      "Train Epoch: 3 [31360/60000 (52%)]\tLoss: 0.329677\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.318600\n",
      "Train Epoch: 3 [32640/60000 (54%)]\tLoss: 0.363977\n",
      "Train Epoch: 3 [33280/60000 (55%)]\tLoss: 0.344746\n",
      "Train Epoch: 3 [33920/60000 (57%)]\tLoss: 0.182738\n",
      "Train Epoch: 3 [34560/60000 (58%)]\tLoss: 0.300196\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.166653\n",
      "Train Epoch: 3 [35840/60000 (60%)]\tLoss: 0.320854\n",
      "Train Epoch: 3 [36480/60000 (61%)]\tLoss: 0.187464\n",
      "Train Epoch: 3 [37120/60000 (62%)]\tLoss: 0.425142\n",
      "Train Epoch: 3 [37760/60000 (63%)]\tLoss: 0.301960\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.186460\n",
      "Train Epoch: 3 [39040/60000 (65%)]\tLoss: 0.347223\n",
      "Train Epoch: 3 [39680/60000 (66%)]\tLoss: 0.129197\n",
      "Train Epoch: 3 [40320/60000 (67%)]\tLoss: 0.283092\n",
      "Train Epoch: 3 [40960/60000 (68%)]\tLoss: 0.285335\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.158976\n",
      "Train Epoch: 3 [42240/60000 (70%)]\tLoss: 0.178819\n",
      "Train Epoch: 3 [42880/60000 (71%)]\tLoss: 0.235058\n",
      "Train Epoch: 3 [43520/60000 (72%)]\tLoss: 0.255371\n",
      "Train Epoch: 3 [44160/60000 (74%)]\tLoss: 0.324273\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.261816\n",
      "Train Epoch: 3 [45440/60000 (76%)]\tLoss: 0.238071\n",
      "Train Epoch: 3 [46080/60000 (77%)]\tLoss: 0.271907\n",
      "Train Epoch: 3 [46720/60000 (78%)]\tLoss: 0.484175\n",
      "Train Epoch: 3 [47360/60000 (79%)]\tLoss: 0.358166\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.394997\n",
      "Train Epoch: 3 [48640/60000 (81%)]\tLoss: 0.208115\n",
      "Train Epoch: 3 [49280/60000 (82%)]\tLoss: 0.143942\n",
      "Train Epoch: 3 [49920/60000 (83%)]\tLoss: 0.481614\n",
      "Train Epoch: 3 [50560/60000 (84%)]\tLoss: 0.302430\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.234462\n",
      "Train Epoch: 3 [51840/60000 (86%)]\tLoss: 0.224697\n",
      "Train Epoch: 3 [52480/60000 (87%)]\tLoss: 0.209032\n",
      "Train Epoch: 3 [53120/60000 (88%)]\tLoss: 0.240810\n",
      "Train Epoch: 3 [53760/60000 (90%)]\tLoss: 0.408816\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.441445\n",
      "Train Epoch: 3 [55040/60000 (92%)]\tLoss: 0.279208\n",
      "Train Epoch: 3 [55680/60000 (93%)]\tLoss: 0.269343\n",
      "Train Epoch: 3 [56320/60000 (94%)]\tLoss: 0.307032\n",
      "Train Epoch: 3 [56960/60000 (95%)]\tLoss: 0.318125\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.164356\n",
      "Train Epoch: 3 [58240/60000 (97%)]\tLoss: 0.100907\n",
      "Train Epoch: 3 [58880/60000 (98%)]\tLoss: 0.196487\n",
      "Train Epoch: 3 [59520/60000 (99%)]\tLoss: 0.340549\n",
      "\n",
      "Test set: Avg. loss: 0.0921, Accuracy: 9716/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "\n",
    "examples = enumerate(test_loader)\n",
    "batch_idx, (example_data, example_targets) = next(examples)\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    \n",
    "network = Net()\n",
    "optimizer = optim.SGD(network.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "test_losses = []\n",
    "test_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]\n",
    "\n",
    "\n",
    "def train(epoch):\n",
    "  network.train()\n",
    "  for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    optimizer.zero_grad()\n",
    "    output = network(data)\n",
    "    loss = F.nll_loss(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % log_interval == 0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), loss.item()))\n",
    "      train_losses.append(loss.item())\n",
    "      train_counter.append(\n",
    "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
    "    \n",
    "      torch.save(network.state_dict(), 'model.pth')\n",
    "      torch.save(optimizer.state_dict(), 'optimizer.pth')\n",
    "    \n",
    "\n",
    "def test():\n",
    "  network.eval()\n",
    "  test_loss = 0\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "      output = network(data)\n",
    "      test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "      pred = output.data.max(1, keepdim=True)[1]\n",
    "      correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "  test_loss /= len(test_loader.dataset)\n",
    "  test_losses.append(test_loss)\n",
    "  print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "test()\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "  train(epoch)\n",
    "  test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution operator - OOP way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's kick off this chapter by using convolution operator from the torch.nn package. You are going to create a random tensor which will represent your image and random filters to convolve the image with. Then you'll apply those images.\n",
    "\n",
    "The torch library and the torch.nn package have already been imported for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Create 10 random images of shape (1, 28, 28)\n",
    "images = torch.rand(10, 1, 28, 28)\n",
    "\n",
    "# Build 6 conv. filters\n",
    "conv_filters = torch.nn.Conv2d(in_channels=1,out_channels=6, kernel_size=3,\n",
    "stride=1, padding=1)\n",
    "\n",
    "# Convolve the image with the filters \n",
    "output_feature = conv_filters(images)\n",
    "print(output_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 6, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Create 10 random images\n",
    "image = torch.rand(10, 1, 28, 28)\n",
    "\n",
    "# Create 6 filters\n",
    "filters = torch.rand(6, 1, 3, 3)\n",
    "\n",
    "# Convolve the image with the filters\n",
    "output_feature = F.conv2d(image, filters,\n",
    "stride=1, padding=1)\n",
    "print(output_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[8., 5., 9.],\n",
      "          [9., 2., 6.],\n",
      "          [2., 9., 8.]]]])\n",
      "tensor([[[[8., 5., 9.],\n",
      "          [9., 2., 6.],\n",
      "          [2., 9., 8.]]]])\n"
     ]
    }
   ],
   "source": [
    "im = torch.tensor([[[[ 8.,  1.,  2.,  5.,  3.,  1.],\n",
    "          [ 6.,  0.,  0., -5.,  7.,  9.],\n",
    "          [ 1.,  9., -1., -2.,  2.,  6.],\n",
    "          [ 0.,  4.,  2., -3.,  4.,  3.],\n",
    "          [ 2., -1.,  4., -1., -2.,  3.],\n",
    "          [ 2., -4.,  5.,  9., -7.,  8.]]]])\n",
    "# Build a pooling operator with size `2`.\n",
    "max_pooling = torch.nn.MaxPool2d(2)\n",
    "\n",
    "# Apply the pooling operator\n",
    "output_feature = max_pooling(im)\n",
    "\n",
    "# Use pooling operator in the image\n",
    "output_feature_F = F.max_pool2d(im, 2)\n",
    "\n",
    "# print the results of both cases\n",
    "print(output_feature)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 3.7500,  0.5000,  5.0000],\n",
      "          [ 3.5000, -1.0000,  3.7500],\n",
      "          [-0.2500,  4.2500,  0.5000]]]])\n",
      "tensor([[[[ 3.7500,  0.5000,  5.0000],\n",
      "          [ 3.5000, -1.0000,  3.7500],\n",
      "          [-0.2500,  4.2500,  0.5000]]]])\n"
     ]
    }
   ],
   "source": [
    "# Build a pooling operator with size `2`.\n",
    "avg_pooling = torch.nn.AvgPool2d(2)\n",
    "\n",
    "# Apply the pooling operator\n",
    "output_feature = avg_pooling(im)\n",
    "\n",
    "# Use pooling operator in the image\n",
    "output_feature_F = F.avg_pool2d(im, 2)\n",
    "\n",
    "# print the results of both cases\n",
    "print(output_feature)\n",
    "print(output_feature_F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(7*7*10, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write the second argument of view() as (height * width * num_channels) instead of (dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "\t\t\n",
    "        # Instantiate the ReLU nonlinearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Instantiate two convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Instantiate a max pooling layer\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # Instantiate a fully connected layer\n",
    "        self.fc = nn.Linear(7 * 7 * 10, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu( self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Apply conv followd by relu, then in next line pool\n",
    "        x = self.relu( self.conv2(x))\n",
    "        x =  self.pool(x)\n",
    "\n",
    "        # Prepare the image for the fully connected layer\n",
    "        x = x.view(-1, 7*7*10)\n",
    "\n",
    "        # Apply the fully connected layer and return the result\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(10)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Compute the forward pass\n",
    "    outputs = net(inputs)\n",
    "        \n",
    "    # Compute the loss function\n",
    "    loss = criterion(outputs, labels)\n",
    "        \n",
    "    # Compute the gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Iterate over the data in the test_loader\n",
    "for data in test_loader:\n",
    "\n",
    "    # Get the image and label from data\n",
    "    image, label = data\n",
    "\n",
    "    # Make a forward pass in the net with your image\n",
    "    output = net(image)\n",
    "\n",
    "    # Argmax the results of the net\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    if predicted == label:\n",
    "        print(\"Yipes, your net made the right prediction \" + str(predicted))\n",
    "    else:\n",
    "        print(\"Your net prediction was \" + str(predicted) + \", but the correct label is: \" + str(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024), nn.ReLU(inplace=True),\n",
    "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(2048, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Declare all the layers for feature extraction\n",
    "        self.features = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=3, padding=1), \n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=5, out_channels=10, kernel_size=3, padding=1), \n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, padding=1),\n",
    "                                      nn.ReLU(inplace=True),\n",
    "                                      nn.Conv2d(in_channels=20, out_channels=40, kernel_size=3, padding=1),\n",
    "                                      nn.MaxPool2d(2, 2), nn.ReLU(inplace=True))\n",
    "        \n",
    "        # Declare all the layers for classification\n",
    "        self.classifier = nn.Sequential(nn.Linear(7 * 7 * 40, 1024), nn.ReLU(inplace=True),\n",
    "                                       \tnn.Linear(1024, 2048), nn.ReLU(inplace=True),\n",
    "                                        nn.Linear(2048, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "      \n",
    "        # Apply the feature extractor in the input\n",
    "        x = self.features(x)\n",
    "        \n",
    "        # Squeeze the three spatial dimensions in one\n",
    "        x = x.view(-1, 7 * 7 * 40)\n",
    "        \n",
    "        # Classify the images\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Shuffle the indices\n",
    "indices = np.arange(60000)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Build the train loader\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                     transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                     batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[:55000]))\n",
    "\n",
    "# Build the validation loader\n",
    "val_loader = torch.utils.data.DataLoader(datasets.MNIST('mnist', download=True, train=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])),\n",
    "                   batch_size=64, shuffle=False, sampler=torch.utils.data.SubsetRandomSampler(indices[55000:]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instantiate the network\n",
    "model = Net()\n",
    "\n",
    "# Instantiate the cross-entropy loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Instantiate the Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Define all the parameters of the net\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(28*28, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(200, 500),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(500, 10))\n",
    "        \n",
    "    def forward(self, x):\n",
    "    \n",
    "    \t# Do the forward pass\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Implement the sequential module for feature extraction\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(10),\n",
    "            nn.Conv2d(in_channels=10, out_channels=20, kernel_size=3, stride=1, padding=1),\n",
    "            nn.MaxPool2d(2, 2), nn.ReLU(inplace=True), nn.BatchNorm2d(20))\n",
    "        \n",
    "        # Implement the fully connected layer for classification\n",
    "        self.fc = nn.Linear(in_features=7*7*20, out_features=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a new model\n",
    "model = Net()\n",
    "\n",
    "# Change the number of output units\n",
    "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Create a new model\n",
    "model = Net()\n",
    "\n",
    "# Load the parameters from the old model\n",
    "model.load_state_dict(torch.load('my_net.pth'))\n",
    "\n",
    "# Change the number of out channels\n",
    "model.fc = nn.Linear(7 * 7 * 512, 26)\n",
    "\n",
    "# Train and evaluate the model\n",
    "model.train()\n",
    "train_net(model, optimizer, criterion)\n",
    "print(\"Accuracy of the net is: \" + str(model.eval()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Import the module\n",
    "import torchvision\n",
    "\n",
    "# Download resnet18\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "\n",
    "# Freeze all the layers bar the last one\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Change the number of output units\n",
    "model.fc = nn.Linear(512, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
